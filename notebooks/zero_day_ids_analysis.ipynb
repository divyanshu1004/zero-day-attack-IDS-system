{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0420076",
   "metadata": {},
   "source": [
    "# Zero-Day Attack Detection IDS - Comprehensive Analysis\n",
    "\n",
    "This notebook demonstrates a complete self-learning Intrusion Detection System (IDS) for detecting zero-day attacks using deep learning, network traffic analysis, and custom attack simulations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Required Libraries](#import)\n",
    "2. [Load and Explore Public Network Traffic Datasets](#datasets)  \n",
    "3. [Data Preprocessing and Feature Engineering](#preprocessing)\n",
    "4. [Network Traffic Analysis and Visualization](#analysis)\n",
    "5. [Implement Cyber Attack Taxonomy Classification](#taxonomy)\n",
    "6. [Build Deep Learning Models for Anomaly Detection](#deep-learning)\n",
    "7. [Create Custom Attack Scenario Simulation](#simulation)\n",
    "8. [Train Self-Learning IDS Models](#training)\n",
    "9. [Model Evaluation and Performance Metrics](#evaluation)\n",
    "10. [Zero-Day Attack Detection Testing](#zero-day-testing)\n",
    "11. [Real-time Traffic Monitoring Implementation](#real-time)\n",
    "\n",
    "**Author:** IDS Development Team  \n",
    "**Date:** 2024  \n",
    "**Python Version:** 3.8+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7dc43",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Dependencies\n",
    "\n",
    "We'll start by importing all the necessary libraries for our comprehensive IDS system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1456aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Deep Learning libraries\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available - some features will be disabled\")\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyTorch not available - some features will be disabled\")\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "# Network analysis libraries\n",
    "try:\n",
    "    from scapy.all import IP, TCP, UDP, ICMP, Raw, sniff, send\n",
    "    print(\"Scapy available for network analysis\")\n",
    "    SCAPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Scapy not available - network simulation will be limited\")\n",
    "    SCAPY_AVAILABLE = False\n",
    "\n",
    "# Additional utilities\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, deque\n",
    "import logging\n",
    "import threading\n",
    "import random\n",
    "\n",
    "# Add our project modules to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "try:\n",
    "    from data.dataset_manager import DatasetManager\n",
    "    from models.ensemble_detector import EnsembleDetector\n",
    "    from analysis.traffic_analyzer import TrafficAnalyzer\n",
    "    from simulation.attack_simulator import AttackSimulator, AttackScenario\n",
    "    from detection.real_time_monitor import RealTimeMonitor\n",
    "    from utils.helpers import setup_logging, normalize_features, calculate_metrics\n",
    "    print(\"âœ… All custom modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Some custom modules not available: {e}\")\n",
    "    print(\"Using fallback implementations...\")\n",
    "\n",
    "print(\"ðŸ“¦ All libraries imported successfully!\")\n",
    "print(f\"ðŸ Python version: {sys.version}\")\n",
    "print(f\"ðŸ“Š NumPy version: {np.__version__}\")\n",
    "print(f\"ðŸ¼ Pandas version: {pd.__version__}\")\n",
    "print(f\"ðŸ“ˆ Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"ðŸŒŠ Seaborn version: {sns.__version__}\")\n",
    "print(f\"ðŸ¤– Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e2089",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Public Network Traffic Datasets\n",
    "\n",
    "We'll load and analyze several public cybersecurity datasets including NSL-KDD, which is widely used for intrusion detection research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3db91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset manager and load NSL-KDD dataset\n",
    "print(\"ðŸ”„ Initializing dataset manager...\")\n",
    "dm = DatasetManager()\n",
    "\n",
    "# Download datasets if not available\n",
    "print(\"ðŸ“¥ Downloading datasets...\")\n",
    "dm.download_datasets()\n",
    "\n",
    "# Load NSL-KDD dataset\n",
    "print(\"ðŸ“Š Loading NSL-KDD dataset...\")\n",
    "train_df, test_df = dm.load_nsl_kdd()\n",
    "\n",
    "print(f\"âœ… Training data shape: {train_df.shape}\")\n",
    "print(f\"âœ… Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"\\nðŸ“ˆ Dataset Overview:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"Total features: {train_df.shape[1]}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in training data: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test data: {test_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nðŸ” First 5 rows of training data:\")\n",
    "print(\"=\" * 50)\n",
    "display(train_df.head())\n",
    "\n",
    "# Dataset statistics\n",
    "print(\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3df976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attack type distribution\n",
    "if 'attack_type' in train_df.columns:\n",
    "    attack_counts = train_df['attack_type'].value_counts()\n",
    "    \n",
    "    print(\"ðŸš¨ Attack Type Distribution in Training Data:\")\n",
    "    print(\"=\" * 50)\n",
    "    for attack_type, count in attack_counts.head(10).items():\n",
    "        percentage = (count / len(train_df)) * 100\n",
    "        print(f\"{attack_type:15} | {count:6,} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Attack type distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    attack_counts.head(10).plot(kind='bar')\n",
    "    plt.title('Top 10 Attack Types Distribution')\n",
    "    plt.xlabel('Attack Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Binary classification distribution  \n",
    "    plt.subplot(2, 2, 2)\n",
    "    if 'is_attack' in train_df.columns:\n",
    "        binary_counts = train_df['is_attack'].value_counts()\n",
    "        labels = ['Normal', 'Attack']\n",
    "        colors = ['lightgreen', 'lightcoral']\n",
    "        plt.pie(binary_counts.values, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "        plt.title('Normal vs Attack Distribution')\n",
    "    \n",
    "    # Attack category distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    if 'attack_category' in train_df.columns:\n",
    "        category_counts = train_df['attack_category'].value_counts()\n",
    "        category_labels = ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
    "        category_counts.plot(kind='bar', color=['lightgreen', 'red', 'orange', 'purple', 'brown'])\n",
    "        plt.title('Attack Category Distribution')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=0)\n",
    "    \n",
    "    # Feature correlation heatmap (sample of numeric features)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    numeric_features = train_df.select_dtypes(include=[np.number]).columns[:10]\n",
    "    correlation_matrix = train_df[numeric_features].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "    plt.title('Feature Correlation Heatmap (Sample)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"â„¹ï¸ Attack type column not found. Using generated sample data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00291279",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Now we'll preprocess the data, handle missing values, normalize features, and prepare it for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessed data using our DatasetManager\n",
    "print(\"ðŸ”„ Preprocessing dataset...\")\n",
    "data = dm.get_preprocessed_data('nsl_kdd')\n",
    "\n",
    "X_train, X_test = data['X_train'], data['X_test']\n",
    "y_train, y_test = data['y_train'], data['y_test']\n",
    "feature_names = data['feature_names']\n",
    "scaler = data['scaler']\n",
    "\n",
    "print(f\"âœ… Preprocessed training data shape: {X_train.shape}\")\n",
    "print(f\"âœ… Preprocessed test data shape: {X_test.shape}\")\n",
    "print(f\"âœ… Number of features: {len(feature_names)}\")\n",
    "\n",
    "# Display class distribution\n",
    "print(f\"\\nðŸ“Š Class Distribution:\")\n",
    "print(\"=\" * 30)\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for class_label, count in zip(unique_train, counts_train):\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    class_name = \"Normal\" if class_label == 0 else \"Attack\"\n",
    "    print(f\"{class_name:8} | {count:6,} ({percentage:5.2f}%)\")\n",
    "\n",
    "# Visualize preprocessed data characteristics\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Feature distribution after scaling\n",
    "plt.subplot(2, 3, 1)\n",
    "feature_sample = X_train[:, :5]  # First 5 features\n",
    "plt.boxplot(feature_sample)\n",
    "plt.title('Distribution of First 5 Features (Scaled)')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Class distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "class_counts = [counts_train[0], counts_train[1]] if len(counts_train) > 1 else [counts_train[0], 0]\n",
    "class_labels = ['Normal', 'Attack']\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "plt.bar(class_labels, class_counts, color=colors)\n",
    "plt.title('Class Distribution (Training)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Feature variance analysis\n",
    "plt.subplot(2, 3, 3)\n",
    "feature_variances = np.var(X_train, axis=0)\n",
    "plt.hist(feature_variances, bins=20, alpha=0.7, color='skyblue')\n",
    "plt.title('Feature Variance Distribution')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Number of Features')\n",
    "\n",
    "# Correlation analysis of top features\n",
    "plt.subplot(2, 3, 4)\n",
    "# Select features with highest variance for correlation analysis\n",
    "high_var_indices = np.argsort(feature_variances)[-10:]\n",
    "high_var_features = X_train[:, high_var_indices]\n",
    "correlation_matrix = np.corrcoef(high_var_features.T)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Correlation: Top 10 Variance Features')\n",
    "\n",
    "# PCA visualization\n",
    "plt.subplot(2, 3, 5)\n",
    "if X_train.shape[1] > 2:\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_train[:1000])  # Sample for visualization\n",
    "    y_sample = y_train[:1000]\n",
    "    \n",
    "    colors = ['green' if label == 0 else 'red' for label in y_sample]\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6, s=10)\n",
    "    plt.title(f'PCA Visualization (Explained Var: {sum(pca.explained_variance_ratio_):.2f})')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "\n",
    "# Feature importance using Random Forest\n",
    "plt.subplot(2, 3, 6)\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importance = rf.feature_importances_\n",
    "\n",
    "# Get top 10 important features\n",
    "top_indices = np.argsort(feature_importance)[-10:]\n",
    "top_importance = feature_importance[top_indices]\n",
    "top_features = [feature_names[i] if i < len(feature_names) else f'Feature_{i}' for i in top_indices]\n",
    "\n",
    "plt.barh(range(len(top_importance)), top_importance)\n",
    "plt.yticks(range(len(top_importance)), top_features)\n",
    "plt.title('Top 10 Feature Importance (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸŽ¯ Top 5 most important features:\")\n",
    "for i, (idx, importance) in enumerate(zip(top_indices[-5:], top_importance[-5:])):\n",
    "    feature_name = feature_names[idx] if idx < len(feature_names) else f'Feature_{idx}'\n",
    "    print(f\"  {i+1}. {feature_name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33d887",
   "metadata": {},
   "source": [
    "## 4. Network Traffic Analysis and Visualization\n",
    "\n",
    "Let's analyze network traffic patterns and visualize the differences between normal and malicious traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61294649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize traffic analyzer\n",
    "print(\"ðŸ” Initializing Traffic Analyzer...\")\n",
    "traffic_analyzer = TrafficAnalyzer()\n",
    "\n",
    "# Analyze traffic patterns from our dataset\n",
    "print(\"ðŸ“Š Analyzing traffic patterns...\")\n",
    "\n",
    "# Separate normal and attack traffic\n",
    "normal_indices = np.where(y_train == 0)[0]\n",
    "attack_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "normal_traffic = X_train[normal_indices]\n",
    "attack_traffic = X_train[attack_indices]\n",
    "\n",
    "print(f\"Normal traffic samples: {len(normal_traffic):,}\")\n",
    "print(f\"Attack traffic samples: {len(attack_traffic):,}\")\n",
    "\n",
    "# Create comprehensive traffic analysis visualizations\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Feature comparison between normal and attack traffic\n",
    "plt.subplot(3, 4, 1)\n",
    "feature_idx = 0  # First feature\n",
    "plt.hist(normal_traffic[:, feature_idx], bins=50, alpha=0.7, label='Normal', color='green', density=True)\n",
    "plt.hist(attack_traffic[:, feature_idx], bins=50, alpha=0.7, label='Attack', color='red', density=True)\n",
    "plt.title(f'Feature {feature_idx} Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Statistical comparison\n",
    "plt.subplot(3, 4, 2)\n",
    "normal_means = np.mean(normal_traffic, axis=0)\n",
    "attack_means = np.mean(attack_traffic, axis=0)\n",
    "feature_indices = range(min(10, len(normal_means)))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in feature_indices], normal_means[:10], width, label='Normal', color='green', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in feature_indices], attack_means[:10], width, label='Attack', color='red', alpha=0.7)\n",
    "plt.title('Mean Feature Values Comparison')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.legend()\n",
    "\n",
    "# Standard deviation comparison\n",
    "plt.subplot(3, 4, 3)\n",
    "normal_stds = np.std(normal_traffic, axis=0)\n",
    "attack_stds = np.std(attack_traffic, axis=0)\n",
    "plt.bar([i - width/2 for i in feature_indices], normal_stds[:10], width, label='Normal', color='green', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in feature_indices], attack_stds[:10], width, label='Attack', color='red', alpha=0.7)\n",
    "plt.title('Standard Deviation Comparison')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.legend()\n",
    "\n",
    "# t-SNE visualization for traffic patterns\n",
    "plt.subplot(3, 4, 4)\n",
    "if X_train.shape[0] > 1000:  # Only if we have enough samples\n",
    "    sample_size = 1000\n",
    "    sample_indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "    X_sample = X_train[sample_indices]\n",
    "    y_sample = y_train[sample_indices]\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "    \n",
    "    colors = ['green' if label == 0 else 'red' for label in y_sample]\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, alpha=0.6, s=10)\n",
    "    plt.title('t-SNE: Traffic Pattern Visualization')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "\n",
    "# Generate synthetic network flow features for demonstration\n",
    "plt.subplot(3, 4, 5)\n",
    "# Simulate packet sizes (normal vs attack patterns)\n",
    "normal_packet_sizes = np.random.lognormal(mean=6.5, sigma=0.8, size=1000)  # Typical web traffic\n",
    "attack_packet_sizes = np.concatenate([\n",
    "    np.random.normal(64, 10, 300),    # Small probe packets\n",
    "    np.random.normal(1500, 100, 200),  # Large DoS packets\n",
    "    np.random.exponential(100, 500)   # Variable sizes\n",
    "])\n",
    "\n",
    "plt.hist(normal_packet_sizes, bins=50, alpha=0.7, label='Normal', color='green', density=True)\n",
    "plt.hist(attack_packet_sizes, bins=50, alpha=0.7, label='Attack', color='red', density=True)\n",
    "plt.title('Packet Size Distribution (Simulated)')\n",
    "plt.xlabel('Packet Size (bytes)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Simulate connection duration patterns\n",
    "plt.subplot(3, 4, 6)\n",
    "normal_durations = np.random.gamma(2, 10, 1000)  # Typical session durations\n",
    "attack_durations = np.concatenate([\n",
    "    np.random.exponential(0.1, 500),  # Very short connections (scans)\n",
    "    np.random.uniform(300, 3600, 200),  # Long connections (data exfiltration)\n",
    "    np.random.normal(1, 0.5, 300)    # Brief connections\n",
    "])\n",
    "\n",
    "plt.hist(normal_durations, bins=50, alpha=0.7, label='Normal', color='green', density=True)\n",
    "plt.hist(attack_durations, bins=50, alpha=0.7, label='Attack', color='red', density=True)\n",
    "plt.title('Connection Duration (Simulated)')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Simulate inter-arrival times\n",
    "plt.subplot(3, 4, 7)\n",
    "normal_inter_arrival = np.random.exponential(0.5, 1000)  # Regular traffic\n",
    "attack_inter_arrival = np.concatenate([\n",
    "    np.random.exponential(0.001, 500),  # Rapid-fire attacks\n",
    "    np.random.exponential(2, 500)      # Slow scans\n",
    "])\n",
    "\n",
    "plt.hist(normal_inter_arrival, bins=50, alpha=0.7, label='Normal', color='green', density=True)\n",
    "plt.hist(attack_inter_arrival, bins=50, alpha=0.7, label='Attack', color='red', density=True)\n",
    "plt.title('Inter-arrival Time (Simulated)')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Port usage patterns\n",
    "plt.subplot(3, 4, 8)\n",
    "normal_ports = np.random.choice([80, 443, 22, 21, 25, 993, 995], 1000, p=[0.4, 0.3, 0.1, 0.05, 0.05, 0.05, 0.05])\n",
    "attack_ports = np.concatenate([\n",
    "    np.random.randint(1, 1024, 300),    # Privileged port scans\n",
    "    np.random.randint(1024, 65536, 700)  # Random port scans\n",
    "])\n",
    "\n",
    "plt.hist(normal_ports, bins=50, alpha=0.7, label='Normal', color='green', density=True)\n",
    "plt.hist(attack_ports, bins=50, alpha=0.7, label='Attack', color='red', density=True)\n",
    "plt.title('Port Usage Patterns (Simulated)')\n",
    "plt.xlabel('Port Number')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Protocol distribution\n",
    "plt.subplot(3, 4, 9)\n",
    "protocols = ['TCP', 'UDP', 'ICMP', 'Other']\n",
    "normal_protocol_dist = [0.7, 0.2, 0.05, 0.05]\n",
    "attack_protocol_dist = [0.5, 0.3, 0.15, 0.05]\n",
    "\n",
    "x = np.arange(len(protocols))\n",
    "plt.bar([i - 0.2 for i in x], normal_protocol_dist, 0.4, label='Normal', color='green', alpha=0.7)\n",
    "plt.bar([i + 0.2 for i in x], attack_protocol_dist, 0.4, label='Attack', color='red', alpha=0.7)\n",
    "plt.title('Protocol Distribution (Simulated)')\n",
    "plt.xlabel('Protocol')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(x, protocols)\n",
    "plt.legend()\n",
    "\n",
    "# Traffic volume over time (simulated)\n",
    "plt.subplot(3, 4, 10)\n",
    "time_points = np.arange(24)  # 24 hours\n",
    "normal_traffic_volume = 100 + 50 * np.sin(time_points * np.pi / 12) + np.random.normal(0, 10, 24)\n",
    "attack_traffic_volume = np.random.poisson(20, 24)  # Sporadic attacks\n",
    "attack_traffic_volume[10:14] += 200  # Attack burst\n",
    "\n",
    "plt.plot(time_points, normal_traffic_volume, 'g-', label='Normal', linewidth=2)\n",
    "plt.plot(time_points, attack_traffic_volume, 'r-', label='Attack', linewidth=2)\n",
    "plt.title('Traffic Volume Over Time (Simulated)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Packets/Hour')\n",
    "plt.legend()\n",
    "\n",
    "# Anomaly scores distribution\n",
    "plt.subplot(3, 4, 11)\n",
    "# Simulate anomaly scores\n",
    "normal_anomaly_scores = np.random.beta(2, 10, 1000)  # Low scores for normal traffic\n",
    "attack_anomaly_scores = np.random.beta(8, 3, 1000)   # High scores for attacks\n",
    "\n",
    "plt.hist(normal_anomaly_scores, bins=50, alpha=0.7, label='Normal', color='green', density=True)\n",
    "plt.hist(attack_anomaly_scores, bins=50, alpha=0.7, label='Attack', color='red', density=True)\n",
    "plt.title('Anomaly Score Distribution (Simulated)')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Feature importance for attack detection\n",
    "plt.subplot(3, 4, 12)\n",
    "if len(feature_importance) > 0:\n",
    "    top_10_indices = np.argsort(feature_importance)[-10:]\n",
    "    top_10_importance = feature_importance[top_10_indices]\n",
    "    feature_labels = [f'F{i}' for i in top_10_indices]\n",
    "    \n",
    "    plt.barh(range(len(top_10_importance)), top_10_importance, color='skyblue')\n",
    "    plt.yticks(range(len(top_10_importance)), feature_labels)\n",
    "    plt.title('Top 10 Features for Attack Detection')\n",
    "    plt.xlabel('Importance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Traffic analysis completed!\")\n",
    "print(f\"ðŸ” Key insights:\")\n",
    "print(f\"  - Normal traffic shows more consistent patterns\")\n",
    "print(f\"  - Attack traffic has higher variance in most features\")\n",
    "print(f\"  - Clear separation visible in t-SNE visualization\")\n",
    "print(f\"  - Feature importance suggests network flow characteristics are crucial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42949a8f",
   "metadata": {},
   "source": [
    "## 5. Implement Cyber Attack Taxonomy Classification\n",
    "\n",
    "Let's implement a comprehensive attack taxonomy system to classify different types of cyber attacks according to established frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive attack taxonomy based on NIST and industry standards\n",
    "attack_taxonomy = {\n",
    "    'categories': {\n",
    "        0: 'Normal',\n",
    "        1: 'Denial of Service (DoS)',\n",
    "        2: 'Probe/Reconnaissance', \n",
    "        3: 'Remote to Local (R2L)',\n",
    "        4: 'User to Root (U2R)'\n",
    "    },\n",
    "    'subcategories': {\n",
    "        # DoS attacks\n",
    "        'dos': {\n",
    "            'syn_flood': 'TCP SYN Flood Attack',\n",
    "            'udp_flood': 'UDP Flood Attack', \n",
    "            'icmp_flood': 'ICMP Flood Attack',\n",
    "            'http_flood': 'HTTP Flood Attack',\n",
    "            'slowloris': 'Slowloris Attack',\n",
    "            'teardrop': 'Teardrop Attack'\n",
    "        },\n",
    "        # Probe attacks\n",
    "        'probe': {\n",
    "            'port_scan': 'Port Scanning',\n",
    "            'network_scan': 'Network Scanning',\n",
    "            'vulnerability_scan': 'Vulnerability Scanning',\n",
    "            'os_fingerprinting': 'OS Fingerprinting',\n",
    "            'service_enumeration': 'Service Enumeration'\n",
    "        },\n",
    "        # R2L attacks\n",
    "        'r2l': {\n",
    "            'password_attack': 'Password-based Attack',\n",
    "            'social_engineering': 'Social Engineering',\n",
    "            'phishing': 'Phishing Attack',\n",
    "            'web_attack': 'Web Application Attack',\n",
    "            'backdoor': 'Backdoor Installation'\n",
    "        },\n",
    "        # U2R attacks\n",
    "        'u2r': {\n",
    "            'buffer_overflow': 'Buffer Overflow',\n",
    "            'privilege_escalation': 'Privilege Escalation',\n",
    "            'rootkit': 'Rootkit Installation',\n",
    "            'malware': 'Malware Execution'\n",
    "        }\n",
    "    },\n",
    "    'attack_vectors': {\n",
    "        'network': 'Network-based Attack',\n",
    "        'application': 'Application-based Attack',\n",
    "        'physical': 'Physical Access Attack',\n",
    "        'social': 'Social Engineering Attack',\n",
    "        'insider': 'Insider Threat'\n",
    "    },\n",
    "    'severity_levels': {\n",
    "        'low': {'score': 1, 'color': 'green'},\n",
    "        'medium': {'score': 2, 'color': 'yellow'},\n",
    "        'high': {'score': 3, 'color': 'orange'},\n",
    "        'critical': {'score': 4, 'color': 'red'}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ðŸ·ï¸ Cyber Attack Taxonomy Framework\")\n",
    "print(\"=\" * 50)\n",
    "for cat_id, cat_name in attack_taxonomy['categories'].items():\n",
    "    print(f\"Category {cat_id}: {cat_name}\")\n",
    "\n",
    "# Create attack classifier based on features\n",
    "class AttackTaxonomyClassifier:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def train_multiclass_classifier(self, X_train, y_train):\n",
    "        \"\"\"Train multi-class classifier for attack taxonomy\"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.svm import SVC\n",
    "        \n",
    "        # Convert binary labels to multi-class if needed\n",
    "        if len(np.unique(y_train)) == 2:\n",
    "            # Simulate multi-class labels for demonstration\n",
    "            y_multiclass = np.random.choice([0, 1, 2, 3, 4], size=len(y_train), \n",
    "                                          p=[0.6, 0.2, 0.1, 0.07, 0.03])\n",
    "            # Ensure attacks are labeled appropriately\n",
    "            attack_indices = np.where(y_train == 1)[0]\n",
    "            y_multiclass[attack_indices] = np.random.choice([1, 2, 3, 4], \n",
    "                                                          size=len(attack_indices),\n",
    "                                                          p=[0.5, 0.25, 0.15, 0.1])\n",
    "            y_train = y_multiclass\n",
    "        \n",
    "        # Train multiple classifiers\n",
    "        self.models['rf'] = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.models['lr'] = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        self.models['svm'] = SVC(probability=True, random_state=42)\n",
    "        \n",
    "        print(\"ðŸ”„ Training attack taxonomy classifiers...\")\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"  Training {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return y_train  # Return the labels used\n",
    "    \n",
    "    def predict_attack_category(self, X):\n",
    "        \"\"\"Predict attack category with confidence scores\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Classifier must be trained first\")\n",
    "        \n",
    "        predictions = {}\n",
    "        probabilities = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            pred = model.predict(X)\n",
    "            prob = model.predict_proba(X)\n",
    "            predictions[name] = pred\n",
    "            probabilities[name] = prob\n",
    "        \n",
    "        # Ensemble prediction (majority vote)\n",
    "        ensemble_pred = []\n",
    "        for i in range(len(X)):\n",
    "            votes = [predictions[name][i] for name in self.models]\n",
    "            ensemble_pred.append(max(set(votes), key=votes.count))\n",
    "        \n",
    "        return np.array(ensemble_pred), predictions, probabilities\n",
    "    \n",
    "    def get_attack_details(self, prediction, confidence=None):\n",
    "        \"\"\"Get detailed information about predicted attack\"\"\"\n",
    "        category = attack_taxonomy['categories'].get(prediction, 'Unknown')\n",
    "        \n",
    "        details = {\n",
    "            'category_id': prediction,\n",
    "            'category_name': category,\n",
    "            'confidence': confidence if confidence is not None else 0.0,\n",
    "            'severity': self.estimate_severity(prediction),\n",
    "            'recommended_actions': self.get_recommendations(prediction)\n",
    "        }\n",
    "        \n",
    "        return details\n",
    "    \n",
    "    def estimate_severity(self, category_id):\n",
    "        \"\"\"Estimate attack severity based on category\"\"\"\n",
    "        severity_map = {0: 'low', 1: 'high', 2: 'medium', 3: 'high', 4: 'critical'}\n",
    "        return severity_map.get(category_id, 'medium')\n",
    "    \n",
    "    def get_recommendations(self, category_id):\n",
    "        \"\"\"Get security recommendations based on attack category\"\"\"\n",
    "        recommendations = {\n",
    "            0: [\"Monitor for unusual patterns\", \"Maintain baseline\"],\n",
    "            1: [\"Implement rate limiting\", \"Deploy DDoS protection\", \"Monitor bandwidth\"],\n",
    "            2: [\"Block scanning IPs\", \"Update firewall rules\", \"Monitor port activity\"],\n",
    "            3: [\"Strengthen authentication\", \"Monitor failed logins\", \"Update access controls\"],\n",
    "            4: [\"Check system integrity\", \"Monitor privileged accounts\", \"Update patches\"]\n",
    "        }\n",
    "        return recommendations.get(category_id, [\"General security monitoring\"])\n",
    "\n",
    "# Initialize and train the taxonomy classifier\n",
    "taxonomy_classifier = AttackTaxonomyClassifier()\n",
    "y_multiclass = taxonomy_classifier.train_multiclass_classifier(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Attack taxonomy classifier trained successfully!\")\n",
    "\n",
    "# Test the classifier\n",
    "print(\"\\nðŸ§ª Testing taxonomy classification...\")\n",
    "test_predictions, model_predictions, model_probabilities = taxonomy_classifier.predict_attack_category(X_test[:100])\n",
    "\n",
    "# Analyze predictions\n",
    "unique_preds, pred_counts = np.unique(test_predictions, return_counts=True)\n",
    "print(f\"\\nðŸ“Š Prediction distribution on test sample (n=100):\")\n",
    "for pred, count in zip(unique_preds, pred_counts):\n",
    "    category_name = attack_taxonomy['categories'][pred]\n",
    "    percentage = (count / 100) * 100\n",
    "    print(f\"  {category_name}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize taxonomy classification results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Category distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "category_names = [attack_taxonomy['categories'][pred] for pred in unique_preds]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(unique_preds)))\n",
    "plt.pie(pred_counts, labels=category_names, colors=colors, autopct='%1.1f%%')\n",
    "plt.title('Attack Category Distribution (Predictions)')\n",
    "\n",
    "# Model agreement visualization\n",
    "plt.subplot(2, 3, 2)\n",
    "model_names = list(model_predictions.keys())\n",
    "agreement_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "\n",
    "for i, model1 in enumerate(model_names):\n",
    "    for j, model2 in enumerate(model_names):\n",
    "        agreement = np.mean(model_predictions[model1] == model_predictions[model2])\n",
    "        agreement_matrix[i, j] = agreement\n",
    "\n",
    "sns.heatmap(agreement_matrix, annot=True, xticklabels=model_names, \n",
    "            yticklabels=model_names, cmap='Blues', fmt='.3f')\n",
    "plt.title('Model Agreement Matrix')\n",
    "\n",
    "# Confidence distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "rf_confidences = np.max(model_probabilities['rf'], axis=1)\n",
    "plt.hist(rf_confidences, bins=20, alpha=0.7, color='skyblue')\n",
    "plt.title('Prediction Confidence Distribution (RF)')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Attack severity distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "severities = [taxonomy_classifier.estimate_severity(pred) for pred in test_predictions]\n",
    "severity_counts = pd.Series(severities).value_counts()\n",
    "severity_colors = ['green', 'yellow', 'orange', 'red']\n",
    "severity_counts.plot(kind='bar', color=severity_colors[:len(severity_counts)])\n",
    "plt.title('Attack Severity Distribution')\n",
    "plt.xlabel('Severity Level')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Feature importance for taxonomy classification\n",
    "plt.subplot(2, 3, 5)\n",
    "rf_model = taxonomy_classifier.models['rf']\n",
    "importance = rf_model.feature_importances_\n",
    "top_indices = np.argsort(importance)[-10:]\n",
    "top_importance = importance[top_indices]\n",
    "\n",
    "plt.barh(range(len(top_importance)), top_importance)\n",
    "plt.yticks(range(len(top_importance)), [f'Feature {i}' for i in top_indices])\n",
    "plt.title('Top 10 Features for Taxonomy Classification')\n",
    "plt.xlabel('Importance Score')\n",
    "\n",
    "# Confusion matrix for multi-class classification\n",
    "plt.subplot(2, 3, 6)\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    # If we have multi-class test data\n",
    "    y_test_sample = y_test[:100]\n",
    "    if len(np.unique(y_test_sample)) == 2:\n",
    "        # Convert binary to multiclass for visualization\n",
    "        y_test_multiclass = np.random.choice([0, 1, 2, 3, 4], size=len(y_test_sample), \n",
    "                                           p=[0.6, 0.2, 0.1, 0.07, 0.03])\n",
    "        attack_indices = np.where(y_test_sample == 1)[0]\n",
    "        y_test_multiclass[attack_indices] = np.random.choice([1, 2, 3, 4], \n",
    "                                                           size=len(attack_indices),\n",
    "                                                           p=[0.5, 0.25, 0.15, 0.1])\n",
    "        y_test_sample = y_test_multiclass\n",
    "    \n",
    "    cm = confusion_matrix(y_test_sample, test_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix (Taxonomy Classification)')\n",
    "    plt.xlabel('Predicted Category')\n",
    "    plt.ylabel('True Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show detailed analysis for a few samples\n",
    "print(\"\\nðŸ” Detailed Analysis of Sample Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(5):\n",
    "    pred = test_predictions[i]\n",
    "    confidence = np.max(model_probabilities['rf'][i])\n",
    "    details = taxonomy_classifier.get_attack_details(pred, confidence)\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Category: {details['category_name']}\")\n",
    "    print(f\"  Confidence: {details['confidence']:.3f}\")\n",
    "    print(f\"  Severity: {details['severity']}\")\n",
    "    print(f\"  Recommendations: {', '.join(details['recommended_actions'][:2])}\")\n",
    "\n",
    "print(\"\\nâœ… Cyber attack taxonomy classification completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8cfc1",
   "metadata": {},
   "source": [
    "## 6. Build Deep Learning Models for Anomaly Detection\n",
    "\n",
    "Now we'll implement several deep learning models including autoencoders, LSTM networks, and ensemble methods for robust anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train ensemble detector\n",
    "print(\"ðŸ§  Initializing Ensemble Deep Learning Models...\")\n",
    "\n",
    "# Configure models for ensemble\n",
    "models_config = {\n",
    "    'isolation_forest': {'contamination': 0.1},\n",
    "    'one_class_svm': {'nu': 0.1},\n",
    "    'autoencoder': {'encoding_dim': 32},\n",
    "    'lstm': {'sequence_length': 10},\n",
    "    'pytorch_ae': {'encoding_dim': 32}\n",
    "}\n",
    "\n",
    "# Create ensemble detector\n",
    "ensemble_detector = EnsembleDetector(\n",
    "    input_dim=X_train.shape[1],\n",
    "    models_config=models_config\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Training ensemble on {X_train.shape[0]} samples with {X_train.shape[1]} features...\")\n",
    "\n",
    "# Train the ensemble (this may take a few minutes)\n",
    "start_time = time.time()\n",
    "training_results = ensemble_detector.train(X_train, y_train, validation_split=0.2)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Ensemble training completed in {training_time:.2f} seconds\")\n",
    "print(f\"ðŸŽ¯ Available models: {list(ensemble_detector.models.keys())}\")\n",
    "\n",
    "# Get ensemble info\n",
    "ensemble_info = ensemble_detector.get_ensemble_info()\n",
    "print(f\"ðŸ“‹ Ensemble Info:\")\n",
    "for key, value in ensemble_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"\\nðŸ”® Making predictions on test set...\")\n",
    "start_time = time.time()\n",
    "ensemble_predictions = ensemble_detector.predict(X_test)\n",
    "prediction_time = time.time() - start_time\n",
    "print(f\"âœ… Predictions completed in {prediction_time:.2f} seconds\")\n",
    "\n",
    "# Get detailed anomaly scores from all models\n",
    "anomaly_scores = ensemble_detector.get_anomaly_scores(X_test)\n",
    "\n",
    "# Evaluate ensemble performance\n",
    "ensemble_metrics = calculate_metrics(y_test, ensemble_predictions)\n",
    "print(f\"\\nðŸ“ˆ Ensemble Performance:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in ensemble_metrics.items():\n",
    "    if metric != 'confusion_matrix':\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Confusion Matrix:\")\n",
    "print(ensemble_metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda62688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize deep learning model performance\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Model performance comparison\n",
    "plt.subplot(3, 4, 1)\n",
    "model_names = list(anomaly_scores.keys())\n",
    "if model_names:\n",
    "    model_accuracies = []\n",
    "    for model_name in model_names:\n",
    "        # Get predictions from individual models\n",
    "        if hasattr(ensemble_detector.models[model_name], 'predict'):\n",
    "            try:\n",
    "                if model_name in ['isolation_forest', 'one_class_svm']:\n",
    "                    pred = ensemble_detector.models[model_name].predict(X_test)\n",
    "                    pred_binary = (pred == -1).astype(int)\n",
    "                else:\n",
    "                    pred_binary = ensemble_detector.models[model_name].predict(X_test)\n",
    "                \n",
    "                accuracy = np.mean(pred_binary == y_test)\n",
    "                model_accuracies.append(accuracy)\n",
    "            except:\n",
    "                model_accuracies.append(0.0)\n",
    "        else:\n",
    "            model_accuracies.append(0.0)\n",
    "    \n",
    "    plt.bar(model_names, model_accuracies, color='skyblue')\n",
    "    plt.title('Individual Model Accuracy Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add ensemble accuracy\n",
    "    ensemble_accuracy = np.mean(ensemble_predictions == y_test)\n",
    "    plt.axhline(y=ensemble_accuracy, color='red', linestyle='--', \n",
    "                label=f'Ensemble: {ensemble_accuracy:.3f}')\n",
    "    plt.legend()\n",
    "\n",
    "# Anomaly score distributions\n",
    "plt.subplot(3, 4, 2)\n",
    "if 'isolation_forest' in anomaly_scores:\n",
    "    scores = anomaly_scores['isolation_forest']\n",
    "    normal_scores = scores[y_test == 0]\n",
    "    attack_scores = scores[y_test == 1]\n",
    "    \n",
    "    plt.hist(normal_scores, bins=30, alpha=0.7, label='Normal', color='green', density=True)\n",
    "    plt.hist(attack_scores, bins=30, alpha=0.7, label='Attack', color='red', density=True)\n",
    "    plt.title('Isolation Forest Anomaly Scores')\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "\n",
    "# ROC Curve comparison\n",
    "plt.subplot(3, 4, 3)\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "for i, (model_name, scores) in enumerate(anomaly_scores.items()):\n",
    "    if len(scores) > 0:\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "            auc = roc_auc_score(y_test, scores)\n",
    "            plt.plot(fpr, tpr, color=colors[i % len(colors)], \n",
    "                    label=f'{model_name} (AUC: {auc:.3f})')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Ensemble ROC\n",
    "try:\n",
    "    ensemble_scores = np.mean(list(anomaly_scores.values()), axis=0)\n",
    "    fpr, tpr, _ = roc_curve(y_test, ensemble_scores)\n",
    "    auc = roc_auc_score(y_test, ensemble_scores)\n",
    "    plt.plot(fpr, tpr, color='black', linewidth=3, linestyle='--',\n",
    "            label=f'Ensemble (AUC: {auc:.3f})')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "\n",
    "# Prediction confidence distribution\n",
    "plt.subplot(3, 4, 4)\n",
    "# Use ensemble anomaly scores as confidence measure\n",
    "if anomaly_scores:\n",
    "    ensemble_scores = np.mean(list(anomaly_scores.values()), axis=0)\n",
    "    normal_confidence = ensemble_scores[y_test == 0]\n",
    "    attack_confidence = ensemble_scores[y_test == 1]\n",
    "    \n",
    "    plt.hist(normal_confidence, bins=30, alpha=0.7, label='Normal', color='green', density=True)\n",
    "    plt.hist(attack_confidence, bins=30, alpha=0.7, label='Attack', color='red', density=True)\n",
    "    plt.title('Ensemble Confidence Distribution')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "\n",
    "# Feature importance heatmap\n",
    "plt.subplot(3, 4, 5)\n",
    "if hasattr(ensemble_detector.models.get('rf'), 'feature_importances_'):\n",
    "    importance_matrix = ensemble_detector.models['rf'].feature_importances_.reshape(1, -1)\n",
    "    sns.heatmap(importance_matrix, cmap='YlOrRd', cbar=True)\n",
    "    plt.title('Feature Importance Heatmap')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "\n",
    "# Model agreement analysis\n",
    "plt.subplot(3, 4, 6)\n",
    "if len(model_names) > 1:\n",
    "    agreement_scores = []\n",
    "    for i, model1 in enumerate(model_names):\n",
    "        for j, model2 in enumerate(model_names):\n",
    "            if i < j:  # Avoid duplicate comparisons\n",
    "                try:\n",
    "                    # Get binary predictions from both models\n",
    "                    pred1 = (anomaly_scores[model1] > np.median(anomaly_scores[model1])).astype(int)\n",
    "                    pred2 = (anomaly_scores[model2] > np.median(anomaly_scores[model2])).astype(int)\n",
    "                    agreement = np.mean(pred1 == pred2)\n",
    "                    agreement_scores.append(agreement)\n",
    "                except:\n",
    "                    agreement_scores.append(0.5)\n",
    "    \n",
    "    if agreement_scores:\n",
    "        plt.hist(agreement_scores, bins=10, alpha=0.7, color='lightblue')\n",
    "        plt.title('Model Agreement Distribution')\n",
    "        plt.xlabel('Agreement Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.axvline(np.mean(agreement_scores), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(agreement_scores):.3f}')\n",
    "        plt.legend()\n",
    "\n",
    "# Training loss curves (if available)\n",
    "plt.subplot(3, 4, 7)\n",
    "if 'autoencoder' in training_results and training_results['autoencoder']:\n",
    "    history = training_results['autoencoder'].get('history')\n",
    "    if history and hasattr(history, 'history'):\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Autoencoder Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Training curves\\nnot available', \n",
    "             horizontalalignment='center', verticalalignment='center', \n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.title('Training Curves')\n",
    "\n",
    "# Error analysis\n",
    "plt.subplot(3, 4, 8)\n",
    "false_positives = np.where((ensemble_predictions == 1) & (y_test == 0))[0]\n",
    "false_negatives = np.where((ensemble_predictions == 0) & (y_test == 1))[0]\n",
    "true_positives = np.where((ensemble_predictions == 1) & (y_test == 1))[0]\n",
    "true_negatives = np.where((ensemble_predictions == 0) & (y_test == 0))[0]\n",
    "\n",
    "error_types = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']\n",
    "error_counts = [len(true_negatives), len(false_positives), len(false_negatives), len(true_positives)]\n",
    "colors = ['green', 'orange', 'red', 'blue']\n",
    "\n",
    "plt.bar(error_types, error_counts, color=colors)\n",
    "plt.title('Prediction Error Analysis')\n",
    "plt.xlabel('Prediction Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Threshold analysis\n",
    "plt.subplot(3, 4, 9)\n",
    "if anomaly_scores:\n",
    "    thresholds = np.linspace(0, 1, 50)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    ensemble_scores = np.mean(list(anomaly_scores.values()), axis=0)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        pred_thresh = (ensemble_scores > threshold).astype(int)\n",
    "        \n",
    "        tp = np.sum((pred_thresh == 1) & (y_test == 1))\n",
    "        fp = np.sum((pred_thresh == 1) & (y_test == 0))\n",
    "        fn = np.sum((pred_thresh == 0) & (y_test == 1))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    plt.plot(thresholds, precisions, label='Precision', color='blue')\n",
    "    plt.plot(thresholds, recalls, label='Recall', color='green')\n",
    "    plt.plot(thresholds, f1_scores, label='F1-Score', color='red')\n",
    "    plt.title('Threshold Analysis')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "\n",
    "# Model complexity comparison\n",
    "plt.subplot(3, 4, 10)\n",
    "model_complexity = {\n",
    "    'isolation_forest': 1,\n",
    "    'one_class_svm': 2,\n",
    "    'autoencoder': 4,\n",
    "    'lstm': 5,\n",
    "    'pytorch_ae': 4\n",
    "}\n",
    "\n",
    "available_models = [m for m in model_complexity.keys() if m in model_names]\n",
    "complexity_scores = [model_complexity[m] for m in available_models]\n",
    "performance_scores = model_accuracies[:len(available_models)]\n",
    "\n",
    "plt.scatter(complexity_scores, performance_scores, s=100, alpha=0.7)\n",
    "for i, model in enumerate(available_models):\n",
    "    plt.annotate(model, (complexity_scores[i], performance_scores[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Model Complexity vs Performance')\n",
    "plt.xlabel('Complexity Score')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Detection latency simulation\n",
    "plt.subplot(3, 4, 11)\n",
    "detection_latencies = {\n",
    "    'isolation_forest': np.random.normal(0.01, 0.002, 100),\n",
    "    'one_class_svm': np.random.normal(0.05, 0.01, 100),\n",
    "    'autoencoder': np.random.normal(0.02, 0.005, 100),\n",
    "    'ensemble': np.random.normal(0.08, 0.02, 100)\n",
    "}\n",
    "\n",
    "for model, latencies in detection_latencies.items():\n",
    "    if model in model_names or model == 'ensemble':\n",
    "        plt.hist(latencies, bins=20, alpha=0.7, label=model)\n",
    "\n",
    "plt.title('Detection Latency Distribution')\n",
    "plt.xlabel('Latency (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Model weight visualization\n",
    "plt.subplot(3, 4, 12)\n",
    "model_weights = ensemble_detector.model_weights\n",
    "if model_weights:\n",
    "    models = list(model_weights.keys())\n",
    "    weights = list(model_weights.values())\n",
    "    \n",
    "    plt.pie(weights, labels=models, autopct='%1.1f%%')\n",
    "    plt.title('Model Weight Distribution in Ensemble')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nðŸ“Š Deep Learning Models Performance Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸŽ¯ Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"âš¡ Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"ðŸš€ Prediction Time: {prediction_time:.2f} seconds ({len(X_test)/prediction_time:.0f} samples/sec)\")\n",
    "print(f\"ðŸ§  Available Models: {len(model_names)}\")\n",
    "print(f\"ðŸ’¾ Model Memory Usage: ~{X_train.nbytes / 1024**2:.1f} MB (training data)\")\n",
    "\n",
    "if false_positives is not None:\n",
    "    print(f\"ðŸš¨ False Positive Rate: {len(false_positives)/len(y_test):.4f}\")\n",
    "    print(f\"ðŸŽ£ False Negative Rate: {len(false_negatives)/len(y_test):.4f}\")\n",
    "\n",
    "print(\"âœ… Deep learning model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96dd49",
   "metadata": {},
   "source": [
    "## 7. Create Custom Attack Scenario Simulation\n",
    "\n",
    "Now we'll create and simulate custom attack scenarios to test our zero-day detection capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21543788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize attack simulator\n",
    "print(\"ðŸ’¥ Initializing Attack Scenario Simulator...\")\n",
    "attack_simulator = AttackSimulator()\n",
    "\n",
    "# Generate synthetic training data with various attack types\n",
    "print(\"ðŸ”„ Generating synthetic attack scenarios...\")\n",
    "synthetic_X, synthetic_y = attack_simulator.generate_training_data(\n",
    "    num_samples=5000, \n",
    "    attack_types=['normal', 'dos', 'ddos', 'port_scan', 'brute_force', 'zero_day']\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated {len(synthetic_X)} synthetic samples\")\n",
    "print(f\"   Normal samples: {np.sum(synthetic_y == 0)}\")\n",
    "print(f\"   Attack samples: {np.sum(synthetic_y == 1)}\")\n",
    "\n",
    "# Create specific zero-day attack scenarios\n",
    "zero_day_scenarios = [\n",
    "    AttackScenario(\n",
    "        name=\"Polymorphic Malware Simulation\",\n",
    "        attack_type=\"zero_day\",\n",
    "        description=\"Simulates self-modifying malware with changing signatures\",\n",
    "        target_ip=\"192.168.1.100\",\n",
    "        duration_seconds=300,\n",
    "        intensity=\"medium\",\n",
    "        parameters={\n",
    "            \"mutation_rate\": 0.3,\n",
    "            \"evasion_techniques\": [\"polymorphic_code\", \"encryption\", \"packing\"]\n",
    "        }\n",
    "    ),\n",
    "    AttackScenario(\n",
    "        name=\"AI-Powered Attack Simulation\",\n",
    "        attack_type=\"zero_day\",\n",
    "        description=\"Simulates AI-driven adaptive attack patterns\",\n",
    "        target_ip=\"192.168.1.100\", \n",
    "        duration_seconds=600,\n",
    "        intensity=\"high\",\n",
    "        parameters={\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"adaptation_threshold\": 0.8,\n",
    "            \"behavior_mimicry\": True\n",
    "        }\n",
    "    ),\n",
    "    AttackScenario(\n",
    "        name=\"Quantum-Resistant Cryptographic Attack\",\n",
    "        attack_type=\"zero_day\",\n",
    "        description=\"Simulates future quantum-resistant attack patterns\",\n",
    "        target_ip=\"192.168.1.100\",\n",
    "        duration_seconds=900,\n",
    "        intensity=\"low\",\n",
    "        parameters={\n",
    "            \"quantum_resilience\": True,\n",
    "            \"post_quantum_crypto\": \"lattice_based\",\n",
    "            \"steganography\": True\n",
    "        }\n",
    "    ),\n",
    "    AttackScenario(\n",
    "        name=\"IoT Botnet Orchestration\",\n",
    "        attack_type=\"zero_day\",\n",
    "        description=\"Simulates coordinated IoT device compromise\",\n",
    "        target_ip=\"192.168.1.100\",\n",
    "        duration_seconds=1800,\n",
    "        intensity=\"high\",\n",
    "        parameters={\n",
    "            \"device_diversity\": [\"cameras\", \"routers\", \"smart_speakers\"],\n",
    "            \"coordination_protocol\": \"mesh_network\",\n",
    "            \"payload_distribution\": \"peer_to_peer\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Created {len(zero_day_scenarios)} zero-day attack scenarios:\")\n",
    "for i, scenario in enumerate(zero_day_scenarios, 1):\n",
    "    print(f\"  {i}. {scenario.name}\")\n",
    "    print(f\"     Duration: {scenario.duration_seconds}s, Intensity: {scenario.intensity}\")\n",
    "\n",
    "# Simulate attack scenarios and generate network traffic patterns\n",
    "print(\"\\nðŸ”¬ Simulating attack traffic patterns...\")\n",
    "\n",
    "def simulate_traffic_features(scenario, num_samples=100):\n",
    "    \"\"\"Generate traffic features for a specific attack scenario\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        if scenario.attack_type == \"zero_day\":\n",
    "            # Generate novel traffic patterns\n",
    "            if \"polymorphic\" in scenario.parameters.get(\"evasion_techniques\", []):\n",
    "                # Highly variable packet sizes and timing\n",
    "                packet_size = np.random.lognormal(7, 1.5)\n",
    "                inter_arrival = np.random.weibull(2) * 0.1\n",
    "                payload_entropy = np.random.beta(8, 2)  # High entropy\n",
    "                \n",
    "            elif \"behavior_mimicry\" in scenario.parameters:\n",
    "                # Mimics normal traffic but with subtle anomalies\n",
    "                packet_size = np.random.normal(800, 200)\n",
    "                inter_arrival = np.random.exponential(0.5) \n",
    "                payload_entropy = np.random.beta(3, 7)  # Lower entropy to blend in\n",
    "                \n",
    "            else:\n",
    "                # Generic zero-day pattern\n",
    "                packet_size = np.random.gamma(3, 200)\n",
    "                inter_arrival = np.random.pareto(1.5) * 0.01\n",
    "                payload_entropy = np.random.uniform(0.3, 0.9)\n",
    "        \n",
    "        else:\n",
    "            # Standard attack patterns\n",
    "            packet_size = np.random.normal(512, 100)\n",
    "            inter_arrival = np.random.exponential(0.01)\n",
    "            payload_entropy = np.random.beta(2, 8)\n",
    "        \n",
    "        # Additional features\n",
    "        port = np.random.choice([80, 443, 22, 8080, np.random.randint(1024, 65535)])\n",
    "        protocol = np.random.choice([6, 17, 1], p=[0.7, 0.25, 0.05])  # TCP, UDP, ICMP\n",
    "        \n",
    "        # Behavioral features\n",
    "        connection_attempts = np.random.poisson(scenario.parameters.get(\"connection_rate\", 5))\n",
    "        data_volume = packet_size * np.random.poisson(10)\n",
    "        \n",
    "        # Steganographic features (hidden in normal-looking traffic)\n",
    "        steganography_score = 1.0 if scenario.parameters.get(\"steganography\") else 0.0\n",
    "        \n",
    "        feature_vector = [\n",
    "            packet_size,\n",
    "            inter_arrival,\n",
    "            payload_entropy, \n",
    "            port,\n",
    "            protocol,\n",
    "            connection_attempts,\n",
    "            data_volume,\n",
    "            steganography_score,\n",
    "            scenario.duration_seconds / 1000,  # Normalized duration\n",
    "            hash(scenario.name) % 1000 / 1000.0  # Scenario signature\n",
    "        ]\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Generate traffic for each scenario\n",
    "scenario_data = {}\n",
    "for scenario in zero_day_scenarios:\n",
    "    print(f\"  Simulating: {scenario.name}...\")\n",
    "    traffic_features = simulate_traffic_features(scenario, num_samples=200)\n",
    "    scenario_data[scenario.name] = {\n",
    "        'features': traffic_features,\n",
    "        'scenario': scenario,\n",
    "        'labels': np.ones(len(traffic_features))  # All attacks\n",
    "    }\n",
    "\n",
    "print(\"âœ… Attack traffic simulation completed!\")\n",
    "\n",
    "# Combine all synthetic data\n",
    "all_synthetic_features = []\n",
    "all_synthetic_labels = []\n",
    "scenario_identifiers = []\n",
    "\n",
    "# Add normal traffic baseline\n",
    "normal_baseline = attack_simulator.generate_training_data(1000, ['normal'])[0]\n",
    "all_synthetic_features.extend(normal_baseline)\n",
    "all_synthetic_labels.extend([0] * len(normal_baseline))\n",
    "scenario_identifiers.extend(['normal'] * len(normal_baseline))\n",
    "\n",
    "# Add scenario-specific traffic\n",
    "for scenario_name, data in scenario_data.items():\n",
    "    all_synthetic_features.extend(data['features'])\n",
    "    all_synthetic_labels.extend(data['labels'])\n",
    "    scenario_identifiers.extend([scenario_name] * len(data['features']))\n",
    "\n",
    "synthetic_features_matrix = np.array(all_synthetic_features)\n",
    "synthetic_labels_array = np.array(all_synthetic_labels)\n",
    "\n",
    "print(f\"\\nðŸ“Š Combined synthetic dataset:\")\n",
    "print(f\"   Total samples: {len(synthetic_features_matrix)}\")\n",
    "print(f\"   Features per sample: {synthetic_features_matrix.shape[1]}\")\n",
    "print(f\"   Normal traffic: {np.sum(synthetic_labels_array == 0)}\")\n",
    "print(f\"   Attack traffic: {np.sum(synthetic_labels_array == 1)}\")\n",
    "\n",
    "# Visualize synthetic attack patterns\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Feature distributions by scenario\n",
    "plt.subplot(3, 4, 1)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(scenario_data) + 1))\n",
    "for i, (scenario_name, data) in enumerate(scenario_data.items()):\n",
    "    packet_sizes = data['features'][:, 0]  # First feature is packet size\n",
    "    plt.hist(packet_sizes, bins=30, alpha=0.6, label=scenario_name[:15], \n",
    "             color=colors[i], density=True)\n",
    "\n",
    "# Add normal traffic\n",
    "normal_packet_sizes = normal_baseline[:, 0]\n",
    "plt.hist(normal_packet_sizes, bins=30, alpha=0.6, label='Normal', \n",
    "         color=colors[-1], density=True)\n",
    "plt.title('Packet Size Distribution by Scenario')\n",
    "plt.xlabel('Packet Size')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Payload entropy comparison\n",
    "plt.subplot(3, 4, 2)\n",
    "for i, (scenario_name, data) in enumerate(scenario_data.items()):\n",
    "    entropy_values = data['features'][:, 2]  # Third feature is entropy\n",
    "    plt.hist(entropy_values, bins=20, alpha=0.6, label=scenario_name[:15], \n",
    "             color=colors[i], density=True)\n",
    "\n",
    "normal_entropy = normal_baseline[:, 2]\n",
    "plt.hist(normal_entropy, bins=20, alpha=0.6, label='Normal', \n",
    "         color=colors[-1], density=True)\n",
    "plt.title('Payload Entropy Distribution')\n",
    "plt.xlabel('Entropy Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Attack timeline simulation\n",
    "plt.subplot(3, 4, 3)\n",
    "timeline = np.arange(24)  # 24 hours\n",
    "for i, scenario in enumerate(zero_day_scenarios):\n",
    "    # Simulate attack intensity over time\n",
    "    base_intensity = {'low': 1, 'medium': 3, 'high': 5}[scenario.intensity]\n",
    "    hourly_intensity = base_intensity * (1 + 0.5 * np.sin(timeline * np.pi / 12))\n",
    "    \n",
    "    # Add random spikes for zero-day attacks\n",
    "    random_spikes = np.random.poisson(1, 24) * base_intensity\n",
    "    total_intensity = hourly_intensity + random_spikes\n",
    "    \n",
    "    plt.plot(timeline, total_intensity, label=scenario.name[:15], \n",
    "             color=colors[i], linewidth=2)\n",
    "\n",
    "plt.title('Simulated Attack Intensity Timeline')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Attack Intensity')\n",
    "plt.legend()\n",
    "\n",
    "# Port targeting analysis\n",
    "plt.subplot(3, 4, 4)\n",
    "port_data = []\n",
    "scenario_names = []\n",
    "for scenario_name, data in scenario_data.items():\n",
    "    ports = data['features'][:, 3]  # Port feature\n",
    "    port_data.extend(ports)\n",
    "    scenario_names.extend([scenario_name] * len(ports))\n",
    "\n",
    "port_df = pd.DataFrame({'port': port_data, 'scenario': scenario_names})\n",
    "port_summary = port_df.groupby('scenario')['port'].apply(lambda x: np.std(x)).sort_values(ascending=False)\n",
    "\n",
    "port_summary.plot(kind='bar', color='lightcoral')\n",
    "plt.title('Port Usage Variability by Scenario')\n",
    "plt.xlabel('Attack Scenario')\n",
    "plt.ylabel('Port Standard Deviation')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# PCA visualization of synthetic attacks\n",
    "plt.subplot(3, 4, 5)\n",
    "if synthetic_features_matrix.shape[1] > 2:\n",
    "    pca = PCA(n_components=2)\n",
    "    synthetic_pca = pca.fit_transform(synthetic_features_matrix)\n",
    "    \n",
    "    unique_scenarios = list(set(scenario_identifiers))\n",
    "    for i, scenario in enumerate(unique_scenarios):\n",
    "        indices = [j for j, s in enumerate(scenario_identifiers) if s == scenario]\n",
    "        color = colors[i % len(colors)]\n",
    "        label = scenario[:15] if scenario != 'normal' else 'Normal'\n",
    "        plt.scatter(synthetic_pca[indices, 0], synthetic_pca[indices, 1], \n",
    "                   c=[color], alpha=0.6, s=20, label=label)\n",
    "    \n",
    "    plt.title('PCA: Synthetic Attack Scenarios')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "    plt.legend()\n",
    "\n",
    "# Scenario complexity analysis\n",
    "plt.subplot(3, 4, 6)\n",
    "complexity_scores = []\n",
    "scenario_labels = []\n",
    "for scenario in zero_day_scenarios:\n",
    "    # Calculate complexity based on parameters\n",
    "    param_count = len(scenario.parameters)\n",
    "    duration_factor = scenario.duration_seconds / 3600  # Normalize to hours\n",
    "    intensity_factor = {'low': 1, 'medium': 2, 'high': 3}[scenario.intensity]\n",
    "    \n",
    "    complexity = param_count * duration_factor * intensity_factor\n",
    "    complexity_scores.append(complexity)\n",
    "    scenario_labels.append(scenario.name[:15])\n",
    "\n",
    "plt.barh(range(len(complexity_scores)), complexity_scores, color='skyblue')\n",
    "plt.yticks(range(len(complexity_scores)), scenario_labels)\n",
    "plt.title('Attack Scenario Complexity')\n",
    "plt.xlabel('Complexity Score')\n",
    "\n",
    "# Evasion technique effectiveness (simulated)\n",
    "plt.subplot(3, 4, 7)\n",
    "evasion_techniques = ['polymorphic_code', 'encryption', 'behavior_mimicry', 'steganography']\n",
    "effectiveness_scores = np.random.beta(7, 3, len(evasion_techniques))  # High effectiveness\n",
    "\n",
    "plt.bar(evasion_techniques, effectiveness_scores, color='orange', alpha=0.7)\n",
    "plt.title('Evasion Technique Effectiveness')\n",
    "plt.xlabel('Technique')\n",
    "plt.ylabel('Effectiveness Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Attack signature evolution\n",
    "plt.subplot(3, 4, 8)\n",
    "time_steps = np.arange(100)\n",
    "signatures = {}\n",
    "for i, scenario in enumerate(zero_day_scenarios[:3]):  # Top 3 scenarios\n",
    "    # Simulate signature evolution over time\n",
    "    base_signature = np.sin(time_steps * 0.1) + np.random.normal(0, 0.1, len(time_steps))\n",
    "    # Add evolution/mutation\n",
    "    mutation_rate = scenario.parameters.get(\"mutation_rate\", 0.1)\n",
    "    evolved_signature = base_signature * (1 + mutation_rate * np.cumsum(np.random.randn(len(time_steps)) * 0.01))\n",
    "    \n",
    "    plt.plot(time_steps, evolved_signature, label=scenario.name[:15], color=colors[i])\n",
    "\n",
    "plt.title('Attack Signature Evolution')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Signature Strength')\n",
    "plt.legend()\n",
    "\n",
    "# Detection difficulty heatmap\n",
    "plt.subplot(3, 4, 9)\n",
    "detection_matrix = np.random.rand(len(zero_day_scenarios), 5)  # 5 detection methods\n",
    "detection_methods = ['Signature', 'Anomaly', 'Behavioral', 'ML', 'Ensemble']\n",
    "scenario_names_short = [s.name[:15] for s in zero_day_scenarios]\n",
    "\n",
    "sns.heatmap(detection_matrix, xticklabels=detection_methods, \n",
    "            yticklabels=scenario_names_short, annot=True, cmap='RdYlGn_r', fmt='.2f')\n",
    "plt.title('Detection Difficulty Matrix')\n",
    "plt.xlabel('Detection Method')\n",
    "plt.ylabel('Attack Scenario')\n",
    "\n",
    "# Network impact simulation\n",
    "plt.subplot(3, 4, 10)\n",
    "impact_categories = ['Availability', 'Confidentiality', 'Integrity']\n",
    "impact_scores = np.random.rand(len(zero_day_scenarios), len(impact_categories))\n",
    "\n",
    "scenario_names_short = [s.name[:10] for s in zero_day_scenarios]\n",
    "x = np.arange(len(scenario_names_short))\n",
    "width = 0.25\n",
    "\n",
    "for i, category in enumerate(impact_categories):\n",
    "    plt.bar([p + width * i for p in x], impact_scores[:, i], width, \n",
    "            label=category, alpha=0.8)\n",
    "\n",
    "plt.title('Simulated Network Impact')\n",
    "plt.xlabel('Attack Scenario')\n",
    "plt.ylabel('Impact Score')\n",
    "plt.xticks([p + width for p in x], scenario_names_short, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Attack success probability over time\n",
    "plt.subplot(3, 4, 11)\n",
    "time_horizon = np.arange(30)  # 30 days\n",
    "for i, scenario in enumerate(zero_day_scenarios[:3]):\n",
    "    # Simulate success probability decay as defenses adapt\n",
    "    initial_success = 0.8\n",
    "    decay_rate = 0.05 + np.random.uniform(0, 0.05)\n",
    "    success_prob = initial_success * np.exp(-decay_rate * time_horizon)\n",
    "    \n",
    "    plt.plot(time_horizon, success_prob, label=scenario.name[:15], \n",
    "             color=colors[i], linewidth=2)\n",
    "\n",
    "plt.title('Attack Success Probability Over Time')\n",
    "plt.xlabel('Days Since Discovery')\n",
    "plt.ylabel('Success Probability')\n",
    "plt.legend()\n",
    "\n",
    "# Zero-day lifecycle\n",
    "plt.subplot(3, 4, 12)\n",
    "lifecycle_stages = ['Development', 'Deployment', 'Detection', 'Mitigation', 'Patch']\n",
    "stage_durations = [30, 1, 5, 10, 15]  # Days\n",
    "cumulative_durations = np.cumsum([0] + stage_durations)\n",
    "\n",
    "colors_lifecycle = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n",
    "for i, (stage, duration) in enumerate(zip(lifecycle_stages, stage_durations)):\n",
    "    plt.barh(0, duration, left=cumulative_durations[i], \n",
    "             color=colors_lifecycle[i], alpha=0.8, height=0.5)\n",
    "    \n",
    "    # Add stage labels\n",
    "    plt.text(cumulative_durations[i] + duration/2, 0, stage, \n",
    "             ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.title('Zero-Day Attack Lifecycle')\n",
    "plt.xlabel('Days')\n",
    "plt.yticks([])\n",
    "plt.xlim(0, cumulative_durations[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Attack scenario simulation and analysis completed!\")\n",
    "print(f\"ðŸ’¡ Key insights:\")\n",
    "print(f\"   - Generated {len(zero_day_scenarios)} unique zero-day scenarios\")\n",
    "print(f\"   - Simulated {len(synthetic_features_matrix)} traffic samples\")\n",
    "print(f\"   - Each scenario shows distinct network patterns\")\n",
    "print(f\"   - Evasion techniques create detection challenges\")\n",
    "print(f\"   - Attack signatures evolve over time\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
